\documentclass[aps,pre,superscriptaddress]{article}

\usepackage{xcolor}
\usepackage{preamble}
\usepackage{color}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{url}
\usepackage{listings}
\graphicspath{ {./figures} }

\begin{document}

% \begin{abstract}
% \end{abstract}

\title{SBM-WCC Ian's Document}
\author{Ian Chen and others}
%\noaffiliation
\date{\today}
\maketitle

\section{Introduction}
See the preprint~\cite{Park25-02}.

\section{Materials and Methods}

\subsection{Software}

\subsubsection*{Chosen SBM}

We primarily use the graph-tool~\cite{graph-tool} software to infer stochastic block models (SBM).
They fit an SBM by minimizing description length (MDL).

We have two procedures for fitting an SBM model, which we call Chosen-Flat and Chosen-Nested SBM --- see the appendix for the commands:
\begin{enumerate}
    \item Fit three SBM models: PP, DC-Flat, and Non-DC-Flat; pick the one with smallest description length
    \item Fit two SBM models: DC-Nested and Non-DC-Nested; pick the one with smallest description length
\end{enumerate}

\subsubsection*{PySBM}

We also ran experiments with the PySBM software~\cite{funke19-04}.
They implement other SBM variants.

\subsubsection*{Leiden}

We also used Leiden~\cite{traag19-03} to generate clusters.
Leiden can optimize for the Constant Potts Model (CPM) or Modularity (Mod).

\subsection{Datasets}

\subsubsection*{Empirical}

We used 74 empirical non-bipartite networks from the Netzschleuder network catalogue~\cite{Netzschleuder}.
We treat each network as undirected.
These range from 906 to 1.4 million nodes.

\subsection{Synthetic}

We generated 308 synthetic networks using EC-SBM~\cite{vule25-02} based on the 74 empirical networks above.
For each of the 74 networks, we clustered it with Leiden or graph-tool using one of the following four methods:
\begin{enumerate}
    \item Leiden optimizing CPM with resolution 0.1
    \item Leiden optimizing CPM with resolution 0.01
    \item Leiden optimizing Mod
    \item Chosen-SBM with CC treatment
\end{enumerate}

\section{PySBM}

Here, we give a brief overview of other SBM models and inference algorithms, and investigate their runtime and connectivity.
See ~\cite{funke19-04} for a more in-depth discussion of the variants that does not mention connectivity nor runtime.

\subsection{Models}

The main variants of SBM are the degree corrected and non-degree corrected SBM models for flat and also hierarchical.
Here, we use the microcanonical formulation~\cite{peixoto17-01}~\cite{peixoto14-03}.
In addition, closed formulas have been derived that speed up the calculation of the likelihood function by including the number of blocks~\cite{come15-12}.
Finally, there are SBM models that specialize on disassociative structures in bipartite graphs~\cite{yen20-09}.
Since this is a subclass of the other models, here we do not investigate it further.

We follow the naming convention in~\cite{funke19-04}.
See the formulas in~\ref{sbm-formulas}.

\subsection{Inference Algorithms}

Inferring the number of blocks and optimizing the partition given a number of blocks is difficult to do exactly.
Above, we investigated Peixoto's Agglomerative Heuristic~\cite{peixoto14-01}, which starts with a set of singleton clusters and merges them.
In addition, there is the Kernighan-Lin algorithm~\cite{kernighan70-02} which starts with a random partition and does a local search through node moves.

\subsection{Results}

Datasets (10 smallest in the medium list, by node count -- 906 to 2115 nodes):
empirical:
\begin{verbatim}
dnc,uni_email,polblogs,faa_routes,netscience,new_zealand_collab
,collins_yeast,interactome_stelzl,bible_nouns,at_migrations    
\end{verbatim}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/pysbm_conn_all.eps}
	\caption[]{
		\textbf{Connectivity of PySBM on medium empirical non-bipartite networks}
		Each subfigure shows the connectivity of three PySBM inference algorithms on each (non-nested) SBM variant.
		The nested versions are omitted because PySBM only HPAH can run on a nested partition.
		For all models besides SCP, DCPU, and DCPUH (where we used the blocks in graph-tool), we used the same number of blocks for PySBM as in chosen-SBM.
		We used the 10 smallest datasets in the medium empriical datasets, from 906 to 2115 nodes.
		DiscFrequency is the proportion of non-singleton clusters that are disconnected.
	}
	\label{fig:pysbm_disc.pdf}
\end{figure}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/pysbm_v_graph_tool_flat.eps}
	% \includegraphics[width=0.9\textwidth]{figures/pysbm_v_graph_tool_nested.pdf}
	\caption[]{
		\textbf{Comparison of PySBM against graph-tool on medium empirical non-bipartite networks}
		The first two rows compare the entropy and disconnected frequencies on non-nested models. %, and the next two rows compare on nested models.
		We used the same number of blocks for PySBM (KL, MHA, PAH) as in graph-tool (gt). % (but not the number of layers in the nested case).
		Left: We calculate the description length according to each model normalized with dividing by graph-tool's description length. 
		Right: We calculate the percentage of non-singleton clusters that are disconnected.
		We see that a worse description length is consistent with more disconnected clusters.
	}
	\label{fig:comparison_flat.pdf}
\end{figure}

\clearpage
\section{Nested SBM on Empirical}

\begin{figure}[ht]
	\centering
	\begin{subfloat}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/empirical_connectivity_unchosen_cluster.pdf}
	\end{subfloat}
	\caption{
        \textbf{Connectivity of Nested and Flat SBM Clusters}.
        On medium empirical non-bipartite networks.
		Each bar is a network, sorted in increasing network size.
		The y-axis shows, among the non-singleton clusters, the percentage of disconnected, poorly connected, and well-connected clusters.
	}
\end{figure}

\begin{figure}[ht]
	\centering
	\begin{subfloat}
		\centering
		\includegraphics[width=0.9\textwidth]{figures/empirical_connectivity_unchosen_node.pdf}
	\end{subfloat}
	\caption{
        \textbf{Connectivity of Nested and Flat SBM Clusters}.
        On medium empirical non-bipartite networks.
		Each bar is a network, sorted in increasing network size.
        The y-axis shows, among the nodes in non-singleton clusters, the percentage of nodes in disconnected, poorly connected, and well-connected clusters.
	}
\end{figure}
 
\clearpage
\section{SBM and Treatment on Synthetic}

See section~\ref{command-threshold} and~\ref{command-cluster} for commands on running the methods and computing accuracy on thresholded clusters by noise and density.

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/estimated_accuracy_leidenmod.pdf}
    \caption[]{\textbf{Accuracy on EC-SBM x Leiden-Mod:} We show clustering accuracy on 40 EC-SBM networks, generated using Leiden-Mod input clusterings. Each row shows a different accuracy metric, and each column shows a different estimation method. Leiden is run with resolution value 0.001. We see that treatment never helps. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/estimated_accuracy_leiden1.pdf}
    \caption[]{\textbf{Accuracy on EC-SBM x Leiden(0.1):} We show clustering accuracy on 40 EC-SBM networks, generated using Leiden(0.1) input clusterings. Each row shows a different accuracy metric, and each column shows a different estimation method. Leiden is run with resolution value 0.001. We see that treatment always helps, except for AGRI. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/estimated_accuracy_leiden01.pdf}
    \caption[]{\textbf{Accuracy on EC-SBM x Leiden(0.01):} We show clustering accuracy on 40 EC-SBM networks, generated using Leiden(0.01) input clusterings. Each row shows a different accuracy metric, and each column shows a different estimation method. Leiden is run with resolution value 0.001. We see that treatment always helps, except for AGRI. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/estimated_accuracy_sbm.pdf}
    \caption[]{\textbf{Accuracy on EC-SBM x SBM+CC:} We show clustering accuracy on 40 EC-SBM networks, generated using Chosen-Flat SBM+CC input clusterings. Each row shows a different accuracy metric, and each column shows a different estimation method. Leiden is run with resolution value 0.001. We see that treatment always helps, except for on AGRI. Moreover, CC is always better or neutral than WCC. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/node_coverage_ecsbm.pdf}
    \caption[]{\textbf{Node Coverage of EC-SBM estimated and groundtruth clusterings:} Each row shows a different set of 40 networks based on the input clustering method, and each column shows a different estimation method (or ground truth). Leiden is run with resolution value 0.001. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/node_coverage_threshold.pdf}
    \caption[]{\textbf{Node Coverage of EC-SBM groundtruth clusterings:} Each row shows a different set of 70 networks based on the input clustering method. We consider only groundtruth nodes with noise (extdegree / totdegree) less than noise, and remove clusters of size less than 10. We see that even with a threshold of 0.50, leidenmod clusters still have high node coverage, compare to the other input methods. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/noise_acc_leiden.1.pdf}
    \caption[]{\textbf{Accuracy on noise thresholded EC-SBM x Leiden(0.1):} 
        Each column shows the accuracy values on 70 EC-SBM networks based Leiden(0.1). 
        We consider only nodes that are in a groundtruth cluster of size at least $1$ or $10$ respectively for each row.
        Similarly, we consider only nodes that are in a groundtruth cluster with noise (external degree / total degree) less than the threshold, i.e. increasing cluster quality from left to right. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/density_acc_leiden.1.pdf}
    \caption[]{\textbf{Accuracy on density thresholded EC-SBM x Leiden(0.1):} 
        Each column shows the accuracy values on 70 EC-SBM networks based Leiden(0.1). 
        We consider only nodes that are in a groundtruth cluster of size at least $1$ or $10$ respectively for each row.
        Similarly, we consider only nodes that are in a groundtruth cluster with density (2m / n(n-1) ) greater than the threshold , i.e. increasing cluster quality from left to right. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/noise_acc_leiden.01.pdf}
    \caption[]{\textbf{Accuracy on noise thresholded EC-SBM x Leiden(0.01):} 
        Each column shows the accuracy values on 70 EC-SBM networks based Leiden(0.01). 
        We consider only nodes that are in a groundtruth cluster of size at least $1$ or $10$ respectively for each row.
        Similarly, we consider only nodes that are in a groundtruth cluster with noise (external degree / total degree) less than the threshold, i.e. increasing cluster quality from left to right. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/density_acc_leiden.01.pdf}
    \caption[]{\textbf{Accuracy on density thresholded EC-SBM x Leiden(0.01):} 
        Each column shows the accuracy values on 70 EC-SBM networks based Leiden(0.01). 
        We consider only nodes that are in a groundtruth cluster of size at least $1$ or $10$ respectively for each row.
        Similarly, we consider only nodes that are in a groundtruth cluster with density (2m / n(n-1) ) greater than the threshold , i.e. increasing cluster quality from left to right. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/noise_acc_leidenmod.pdf}
    \caption[]{\textbf{Accuracy on noise thresholded EC-SBM x Leiden-Mod:} 
        Each column shows the accuracy values on 70 EC-SBM networks based Leiden-Mod. 
        We consider only nodes that are in a groundtruth cluster of size at least $1$ or $10$ respectively for each row.
        Similarly, we consider only nodes that are in a groundtruth cluster with noise (external degree / total degree) less than the threshold, i.e. increasing cluster quality from left to right. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/density_acc_leidenmod.pdf}
    \caption[]{\textbf{Accuracy on density thresholded EC-SBM x Leiden-Mod:} 
        Each column shows the accuracy values on 70 EC-SBM networks based Leiden-Mod. 
        We consider only nodes that are in a groundtruth cluster of size at least $1$ or $10$ respectively for each row.
        Similarly, we consider only nodes that are in a groundtruth cluster with density (2m / n(n-1) ) greater than the threshold , i.e. increasing cluster quality from left to right. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/noise_acc_sbm.pdf}
    \caption[]{\textbf{Accuracy on noise thresholded EC-SBM x Chosen-Flat+CC:} 
        Each column shows the accuracy values on 70 EC-SBM networks based Chosen-Flat+CC. 
        We consider only nodes that are in a groundtruth cluster of size at least $1$ or $10$ respectively for each row.
        Similarly, we consider only nodes that are in a groundtruth cluster with noise (external degree / total degree) less than the threshold, i.e. increasing cluster quality from left to right. }
\end{figure}

\begin{figure}[!htpb]
    \centering
    \includegraphics[width=0.9\textwidth]{figures/density_acc_sbm.pdf}
    \caption[]{\textbf{Accuracy on density thresholded EC-SBM x Chosen-Flat+CC:} 
        Each column shows the accuracy values on 70 EC-SBM networks based Chosen-Flat+CC. 
        We consider only nodes that are in a groundtruth cluster of size at least $1$ or $10$ respectively for each row.
        Similarly, we consider only nodes that are in a groundtruth cluster with density (2m / n(n-1) ) greater than the threshold , i.e. increasing cluster quality from left to right. }
\end{figure}

\clearpage
\section{LFR and RECCS}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/accuracy_box.pdf}
	\caption[]{\textbf{Clustering accuracy on LFR and RECCS synthetic networks}  Each subfigure shows the results on five different clustering accuracy metrics (NMI, ARI, AGRI, RMI, and AMI). The biggest differences in accuracies come from ARI, AGRI, and RMI, where chosen SBM + WCC has the best accuracy followed by chosen SBM and then hierarchical SBM. On NMI and AMI, although there may be a slight advantage to using chosen SBM + WCC, the accuracy differences across the different methods were small. Notes: SBM+WCC timeout on cit\_patents-0.5(LFR), SBM+WCC oom on wiki\_topcats-0.01(RECCS), and nested SBM oom on oc-0.001(LFR). Data only shown for those datasets where all methods completed successfully. Total of 27 networks.}
	\label{fig:synthetic-accuracy}
\end{figure}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/conn_box.pdf}
	\caption[]{\textbf{Proportion of disconnected clusters produced by Hierarchical SBM and Chosen SBM on synthetic LFR and RECCS networks}  We see that hierarchical SBM tends to produce clusterings with higher proportions of disconnectd clusters than chosen SBM on these synthetic networks. Notes: SBM+WCC timeout on cit\_patents-0.5(LFR), SBM+WCC oom on wiki\_topcats-0.01(RECCS), and nested SBM oom on oc-0.001(LFR). Data only shown for those datasets where all methods completed successfully. Total of 27 synthetic networks.}
	\label{fig:synthetic-connectivity}
\end{figure}

\clearpage
\section{Results on SNAP networks: top 5000 clusters}

This next section has the results on SNAP networks, when we restrict attention to the top 5000 clusters.

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/density_dblp_accuracy.pdf}
	\caption[]{\textbf{Various Clustering Accuracies for Varying Density Levels on the DBLP SNAP real-world network with ground-truth}}
	\label{fig:density-dblp-accuracy}
\end{figure}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/noise_dblp_accuracy.pdf}
	\caption[]{\textbf{Various Clustering Accuracies for Varying Noise Levels on the DBLP SNAP real-world network with ground-truth}}
	\label{fig:noise-dblp-accuracy}
\end{figure}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/density_livejournal_accuracy.pdf}
	\caption[]{\textbf{Various Clustering Accuracies for Varying Density Levels on the Livejournal SNAP real-world network with ground-truth}}
	\label{fig:density-livejournal-accuracy}
\end{figure}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/noise_livejournal_accuracy.pdf}
	\caption[]{\textbf{Various Clustering Accuracies for Varying Noise Levels on the Livejournal SNAP real-world network with ground-truth}}
	\label{fig:noise-livejournal-accuracy}
\end{figure}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/density_youtube_accuracy.pdf}
	\caption[]{\textbf{Various Clustering Accuracies for Varying Density Levels on the Youtube SNAP real-world network with ground-truth}}
	\label{fig:denisty-youtube-accuracy}
\end{figure}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/noise_youtube_accuracy.pdf}
	\caption[]{\textbf{Various Clustering Accuracies for Varying Noise Levels on the Youtube SNAP real-world network with ground-truth}}
	\label{fig:noise-youtube-accuracy}
\end{figure}


\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/density_amazon_accuracy.pdf}
	\caption[]{\textbf{Various Clustering Accuracies for Varying Density Levels on the Amazon SNAP real-world network with ground-truth}}
	\label{fig:density-amazon-acuracy}
\end{figure}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/noise_amazon_accuracy.pdf}
	\caption[]{\textbf{Various Clustering Accuracies for Varying Noise Levels on the Amazon SNAP real-world network with ground-truth}}
	\label{fig:noise-amazon-acuracy}
\end{figure}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/density_orkut_accuracy.pdf}
	\caption[]{\textbf{Various Clustering Accuracies for Varying Density Levels on the Orkut SNAP real-world network with ground-truth}}
	\label{fig:density-orkut-acuracy}
\end{figure}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/noise_orkut_accuracy.pdf}
	\caption[]{\textbf{Various Clustering Accuracies for Varying Noise Levels on the Orkut SNAP real-world network with ground-truth}}
	\label{fig:noise-orkut-acuracy}
\end{figure}

\begin{table}[!htpb]
	\caption{\textbf{Cluster statistics of the top 5000 clusters of SNAP networks}}
	\begin{tabular}{lcrcrr}
		\hline
		dataset     & \multicolumn{2}{c}{\# edges per cluster} & \multicolumn{2}{c}{\# nodes per cluster} & \multicolumn{1}{c}{\# clusters (n \textgreater 100)}                                \\
		\hline

		            & min/median/max                           & mean                                     & min/median/max                                       & mean  & \multicolumn{1}{l}{} \\
		\hline
		\hline
		amazon      & 3/20.0/900                               & 39.8                                     & 3/8.0/328                                            & 13.5  & 31                   \\
		dblp        & 7/21.0/22699                             & 69.1                                     & 6/8.0/7556                                           & 22.4  & 54                   \\
		livejournal & 3/100.0/84155                            & 570.5                                    & 3/16.0/1441                                          & 27.8  & 145                  \\
		orkut       & 3/1590.5/156461                          & 3334.0                                   & 3/115.0/4785                                         & 215.7 & 2769                 \\
		youtube     & 1/3.0/9070                               & 30.4                                     & 2/4.0/2217                                           & 14.6  & 105                  \\
		\hline
	\end{tabular}
\end{table}

\begin{figure}[!htpb]
	\centering
	\includegraphics[width=0.9\textwidth]{figures/snap_groundtruth_top_5000_stats.pdf}
	\caption[]{\textbf{Network and Cluster Statistics for Top 5000 Ground-truth Clusters for the SNAP Collection} Top left: Number of edges and nodes in the network. Top center: Average of the mixing parameters across all nodes in the network and the node coverage of top-5000 ground-truth clusters. Top right: Distribution of edge and node counts over clusters of ground-truth. Bottom left: Distribution of conductance, density, and modularity over clusters of ground-truth. Bottom right: Distribution of normalized mincut sizes over clusters of ground-truth Red horizontal line indicates y=1, which is the threshold for well-connectedness.}
	\label{fig:XXX}
\end{figure}

\clearpage
\section{Formulas for cluster statistics}
Let $V$ be the set of all nodes in the network, $E$ be the set of all edges in the network, and $C$ be the mapping of node IDs to clusters IDs.

Assume that all edges are undirected and are only defined by two vertices $v_{i}$ and $v_{j}$, written as $e_{ij}$ so $e_{ij} = e_{ji}$.

The degree of a node $v_{i}$ is computed by $|\{e_{ij} :v_{i},v_{j} \in V \text{ and } i \neq j\}|$

All of the following calculations are done for a given cluster $c$ which is the set of nodes $\{v_{i} : C(v_{i}) = c\}$.

The sum of all node degrees in cluster $c$ is $k_{c} = \sum_{v_{i} \in c}{degree(v_{i})}$

Similarly, we can define $k_{c_{in}} = 2|\{e_{ij} : v_{i} \in c \text{ and } v_{j} \in c \text{ and } i \neq j\}|$ and $k_{c_{out}} = |\{e_{ij} : v_{i} \in c \text { and } v_{j} \notin c \text{ and } i \neq j\}|$

% $E_{cut}=\{e_{ij} : v_{i}\in c \text{ and } v_{j} \notin c \ \text{ and } i \neq j\}$

\vspace{.1in}
\textcolor{red}{Tandy's comments follow. }

\begin{itemize}

	\item It seems that $E_{cut}$ is exactly the same as $k_{c\_out}$. If so, just use one term and not the other.
	\item It looks like the denominator in Noise is exactly the same as $k_c$.
	\item Tandy;s comments about conductance are probably wrong. Ignore this.
	      'm looking right now at a definition for Conductance in another paper, see Section 2.1 in \url{http://snap.stanford.edu/class/cs224w-2018/reports/CS224W-2018-50.pdf}.    \textcolor{blue}{Min writes:I don't think by definition $k_{c} \leq |E|$ holds true since $k_{c}$ is upper-bounded by $2|E|$. We are double counting edges when summing the degree per node. I also put two subsections below that show that these two formulas end up giving the same numbers. I'm not confident that I didn't make any mistakes but intuitively I feel that $A(S)=k_{S}$ since both of them are counting the number of edges per node in $S$.}
	\item The definition of mixing parameter that you are using is not standard.  You are defining this term in order to have a way of talking about the ``mixing parameter of a cluster", but a different definition might be better. We can discuss.
	      But given the definition, it looks to be exactly the same as Noise.
	      Consider instead just writing down the distribution of individual mixing parameters of the nodes that are in the cluster.
	      That distribution can be then combined to
	      get the distribution of individual mixing
	      parameters for the clusters being considered.
\end{itemize}.



\subsection{Conductance}
A cluster $c$ defines a cut in which on one side of the cut is $c$ and the other side of the cut is $V \setminus c$. Therefore, the number of edges crossing the cut is simply $k_{c_{out}}$.


\begin{equation}
	Conductance(c) = \frac{k_{c_{out}}}{min(k_{c}, 2|E|-k_{c})}
\end{equation}

\subsubsection{Derivation of $A(S)$}
$A_{ij}$ is 1 if there is an edge between $v_{i}$ and $v_{j}$ and 0 otherwise since our input networks are considered unweighted.

\begin{align}
	A(S) & = \sum_{i \in S,j\in V}{A_{ij}}                                                                \\
	     & = \sum_{v_{i} \in S}{|{e_{ij} : v_{i} \in S \text{ and } v_{j} \in V \text { and } i \neq j}|} \\
	     & = \sum_{v_{i} \in S}{degree(v_{i})}
\end{align}

\subsubsection{Derivation of $A(\overline{S})$}
$A_{ij}$ is 1 if there is an edge between $v_{i}$ and $v_{j}$ and 0 otherwise since our input networks are considered unweighted.

\begin{align}
	A(\overline{S}) & = \sum_{i \in \overline{S},j\in V}{A_{ij}}                                                                           \\
	                & = \sum_{v_{i} \in \overline{S}}{|{e_{ij} : v_{i} \in \overline{S} \text{ and } v_{j} \in V \text { and } i \neq j}|} \\
	                & = \sum_{v_{i} \in \overline{S}}{degree(v_{i})}                                                                       \\
	                & = 2|E| - \sum_{v_{i} \in S}{degree(v_{i})}
\end{align}


\subsection{Noise}

\begin{equation}
	Noise = \frac{k_{c\_out}}{k_{c}}
\end{equation}

\subsection{Mixing parameter}
Quickly quoting LFR, ``2 Each node shares a fraction 1-$\mu$ of its links with the other nodes of its community and a fraction $\mu$ with the other nodes of the network; $\mu$ is the mixing parameter.''

The internal degree of a node $v_{i} \in c$ is $k_{v_{i}\_{in}}=|\{e_{ij} :v_{j} \in c \text{ and } i \neq j|$

The external degree of a node $v_{i} \in c$ is $k_{v_{i}\_{out}}=|\{e_{ij} :v_{j} \notin c |$

\begin{equation}
	MixingParameter(v_{i}) = \frac{k_{v_{i}\_out}}{k_{v_{i}\_in} + k_{v_{i}\_out}}
\end{equation}

\subsection{Density}
\begin{equation}
	Density=\frac{|\{e_{ij} : v_{i},v_{j} \in c \text{ and } i \neq j\}|}{{\binom{|N|}{2}}}
\end{equation}

\subsection{Normalized mincut size}
\begin{equation}
	NormalizedMincutSize=\frac{MincutSize}{\log_{10}{|c|}}
\end{equation}
% def conductance(self, graph):
%       num = self.get_border_edges(graph)
%       deg_sum = sum(graph.degree(v) for v in self.nodeset)
%       den = min(deg_sum, 2*graph.m() - deg_sum)
%       if den == 0:
%           # 1. singlenode cluster
%           # 2. entire graph is one cluster
%           # 3. cluster has no intracluster edges
%           return -42
%       return num/den

% \begin{figure}[!htpb]
% \centering
% \includegraphics[]{}
% \caption[]{\textbf{}}
% \label{fig:}
% \end{figure}

\section{Commands}

\subsection{Clustering}
\label{command-cluster}

The codes \verb+run_flat_sbm.py+, \verb+run_nested_sbm.py+, \verb+chosen_flat.py+, and \verb+chosen_nested.py+ can be found at \url{https://github.com/illinois-or-research-analytics/network-analysis-code}, commit ac9af43.
Otherwise, \verb+constrained_clustering+ is at \url{https://github.com/MinhyukPark/constrained-clustering}, release v1.1.0.
Otherwise, \verb+cm_pipeline+ is at \url{https://github.com/illinois-or-research-analytics/cm_pipeline}, release v4.0.1.

\begin{lstlisting}[language=Python]
if method in ["DC-Flat", "NDC-Flat", "PP-Flat"]:
    model = method.split("-")[0].replace("NDC", "Non-DC") + "-SBM"
    return [
        "/usr/bin/time", "-v", "python",
        "run_flat_sbm.py",
        "--input-network", network,
        "--inner-sbm-model", model,
        "--output-prefix", <>,
        "--output-clustering", <>,
        "--num-processors", f"{num_processors}",
    ]
elif method in ["DC-Nested", "NDC-Nested"]:
    model = method.split("-")[0].replace("NDC", "Non-DC") + "-SBM"
    return [
        "/usr/bin/time", "-v", "python",
        "run_nested_sbm.py",
        "--input-network", network,
        "--inner-sbm-model", model,
        "--output-prefix", <>,
        "--output-clustering", <>,
        "--num-processors", f"{num_processors}"
    ]
elif method == "Leiden":
    return [
        "/usr/bin/time", "-v",
        "python", 
        "cm_pipeline/scripts/run_leiden.py",
        "-i", network,
        "-r", "0.001",
        "-o", <>,
        "-n", "2"
    ]
elif method == "Chosen-Nested":
    return [
        "/usr/bin/time", "-v",
        "python",
        "chosen_nested.py",
        "--dataset", dataset,
        "--name", name,
    ]
elif method == "Chosen-Flat":
    return [
        "/usr/bin/time", "-v",
        "python",
        "chosen_flat.py",
        "--dataset", dataset,
        "--name", name,
    ]
elif "+CC" in method:
    return [
        "/usr/bin/time", "-v",
        "constrained_clustering", 
        "MincutOnly",
        "--edgelist", network,
        "--existing-clustering", <>,
        "--output-file", <>,
        "--log-file", <>,
        "--connectedness-criterion", "0",
        "--log-level", "1"
    ]
elif "+WCC" in method:
    return [
        "/usr/bin/time", "-v",
        "constrained_clustering", 
        "MincutOnly",
        "--edgelist", network,
        "--existing-clustering", <>,
        "--output-file", <>,
        "--log-file", <>,
        "--connectedness-criterion", "1",
        "--log-level", "1"
    ]
elif "+CM" in method:
    return [
        "/usr/bin/time", "-v",
        "python", "-m",
        "hm01.cm",
        "-i", network,
        "-e", <>,
        "-o", <>,
        "-c", "leiden",
        "-g", "0.001",
        "--threshold", "1log10",
        "--nprocs", f"{num_processors}"
    ]
\end{lstlisting}

\subsection{Noise and Density}
\label{command-threshold}

The codes can be found at \url{https://github.com/illinois-or-research-analytics/network-analysis-code}, commit ac9af43.
\begin{lstlisting}
python analysis_threshold/noise.py \
    --network <network> \
    --cluster <cluster> \
    --outfile <output>

python analysis_threshold/noise.py \
    --network <network> \
    --cluster <cluster> \
    --outfile <output>

python analysis_threshold/accuracy.py \
    --network <network> \
    --gt_cluster <noise-output or density-output> \
    --et_cluster <cluster> \
    --output <output> \
    --greater <True for density, False for noise> \
    --minsize <1 or 10>
\end{lstlisting}

\section{Description Length Formulas}
\label{sbm-formulas}

\subsection{Microcanonical Non-Degree Corrected (SPC)}

Let $A$ be the adjacency matrix, $\vb{e}$ the edge matrix between blocks, and $\vb{b}$ the block assignments.
We optimize for minimum description length $\Sigma_{spc} = -\log_2P(A, \vb{e}, \vb{b}) = -\log_2 P(A \mid \vb{e}, \vb{b}) - \log_2 P(\vb{e} \mid \vb{b}) - \log_2 P(\vb{b})$, where
\begin{align}
	P(A \mid \vb{e}, \vb{b}) & = \frac{\prod_{r < s} e_{rs}! \prod_{r} e_{rr}!!}{\prod_r n_r^{e_r}} \frac{1}{\prod_{i < j} A_{ij}! \prod_{i} A_{ii}!!} \\
	P(\vb{e} \mid \vb{b})    & = \left( \binom{K(K+1)/2}{M} \right)^{-1}                                                                               \\
	P(\vb{b})                & = \frac{\prod_r n_r!}{N!} \binom{N - 1}{M - 1}^{-1} \frac{1}{N}
\end{align}
where $N$ is the total number of nodes, $M$ the total number of edges, $K$ the number of blocks, $\left( \binom{n}{k} \right) = \binom{n + k - 1}{k}$, and $(2n)!! = 2^n n!$.
See equations (23-25) of ~\cite{peixoto17-01}, or (13-15) of ~\cite{funke19-04}.

In graph-tool, this corresponds to the parameters
\begin{verbatim}
    BlockState(g, b, deg_corr=False).entropy()
\end{verbatim}

\subsection{Microcanonical Degree Corrected (DCPU)}

Let $A$ be the adjacency matrix, $\vb{e}$ the edge matrix between blocks, $\vb{k}$ the degree vector (per node), and $\vb{b}$ the block assignments (per node).
We optimize for minimum description length $\Sigma_{dpcu} = -\log_2P(A, \vb{k}, \vb{e}, \vb{b}) = -\log_2P(A \mid \vb{k}, \vb{e}, \vb{b}) - \log_2 P(\vb{k} \mid \vb{e}, \vb{b}) - \log_2 P(\vb{e} \mid \vb{b}) - \log_2 P(\vb{b}) $, where
\begin{align}
	P(A \mid \vb{k}, \vb{e}, \vb{b}) & = \frac{ \prod_i k_i! \prod_{r < s} e_{rs}! \prod_{r} e_{rr}!!} {\prod_{i < j} A_{ij}! \prod_i A_{ii}!! \prod_r e_r!}                                      \\
	P(\vb{k} \mid \vb{e}, \vb{b})    & = \prod_r \left( \binom{n_r}{e_r} \right)^{-1}                                                                                                             \\
	P(\vb{e} \mid \vb{b})            & = \prod_{l=1}^{L} \prod_{r < s} \left( \binom{n_r^l n_s^l}{e_{rs}^{l+1}} \right)^{-1} \prod_r \left( \binom{ n_r^l (n_r^l + 1) / 2 }{e_{rr}^{l+1} }\right) \\
	P(\vb{b})                        & = \prod_{l=1}^L \frac{\prod_r n_r^l!}{K^{l-1}!} \binom{ K^{l-1} - 1}{K^l - 1}^{-1} \frac{1}{K^{l-1}}
\end{align}
where $N$ is the total number of nodes, $M$ the total number of edges, $K$ the number of blocks, $\left( \binom{n}{k} \right) = \binom{n + k - 1}{k}$, and $(2n)!! = 2^n n!$.
See equations (1-3), (26) in ~\cite{peixoto17-01}, or (16-18) of ~\cite{funke19-04}.

In graph-tool, this corresponds to the parameters
\begin{verbatim}
    BlockState(g, b, deg_corr=False, entropy_args={ degree_dl_kind = "uniform" }).entropy()
\end{verbatim}

\subsection{Microcanonical Degree Corrected (DCPUH)}

Let $A$ be the adjacency matrix, $\vb{e}$ the edge matrix between blocks, $\vb{k}$ the degree vector (per node), and $\vb{b}$ the block assignments (per node).
We optimize for minimum description length $\Sigma_{dpcuh} = -\log_2P(A, \vb{k}, \vb{e}, \vb{b}) = -\log_2P(A \mid \vb{k}, \vb{e}, \vb{b}) - \log_2 P(\vb{k} \mid \vb{e}, \vb{b}) - \log_2 P(\vb{e} \mid \vb{b}) - \log_2 P(\vb{b}) $, where
\begin{align}
	P(A \mid \vb{k}, \vb{e}, \vb{b}) & = \frac{ \prod_i k_i! \prod_{r < s} e_{rs}! \prod_{r} e_{rr}!!} {\prod_{i < j} A_{ij}! \prod_i A_{ii}!! \prod_r e_r!}                                      \\
	P(\vb{k} \mid \vb{e}, \vb{b})    & = \prod_r \frac{\prod_k N_k^r!}{n_r!} q(e_r, n_r)^{-1}                                                                                                     \\
	P(\vb{e} \mid \vb{b})            & = \prod_{l=1}^{L} \prod_{r < s} \left( \binom{n_r^l n_s^l}{e_{rs}^{l+1}} \right)^{-1} \prod_r \left( \binom{ n_r^l (n_r^l + 1) / 2 }{e_{rr}^{l+1} }\right) \\
	P(\vb{b})                        & = \prod_{l=1}^L \frac{\prod_r n_r^l!}{K^{l-1}!} \binom{ K^{l-1} - 1}{K^l - 1}^{-1} \frac{1}{K^{l-1}}
\end{align}
where $N_k^r$ denotes the number of nodes with degree $k$ in group $r$ and $q(m, n)$ the number of partitions of $m$ into $n$ parts.
That is,
\begin{equation}
	q(m, n) =
	\begin{cases}
		1                         & m = n = 0        \\
		0                         & n = 0, m > 0     \\
		q(m, m)                   & n > m            \\
		q(m, n - 1) + q(m - n, n) & \text{otherwise}
	\end{cases}
\end{equation}
See equations (1-3), (27-30) in ~\cite{peixoto17-01}, or (16-18) of ~\cite{funke19-04}.

In graph-tool, this corresponds to the parameters
\begin{verbatim}
    BlockState(g, b, deg_corr=False, entropy_args={ degree_dl_kind = "distributed" }).entropy()
\end{verbatim}

\subsection{Microcanonical Non-Degree Corrected (HSPC)}

Let $A$ be the adjacency matrix, $\vb{e}$ the edge matrix between blocks, and $\vb{b}$ the block assignments.
We optimize for minimum description length $\Sigma_{hspc} = -\log_2P(A, \vb{e}, \vb{b}) = -\log_2 P(A \mid \vb{e}, \vb{b}) - \log_2 P(\vb{e} \mid \vb{b}) - \log_2 P(\vb{b})$, where
\begin{align}
	P(A \mid \vb{e}, \vb{b}) & = \frac{\prod_{r < s} e_{rs}! \prod_{r} e_{rr}!!}{\prod_r n_r^{e_r}} \frac{1}{\prod_{i < j} A_{ij}! \prod_{i} A_{ii}!!}                                    \\
	P(\vb{e} \mid \vb{b})    & = \prod_{l=1}^{L} \prod_{r < s} \left( \binom{n_r^l n_s^l}{e_{rs}^{l+1}} \right)^{-1} \prod_r \left( \binom{ n_r^l (n_r^l + 1) / 2 }{e_{rr}^{l+1} }\right) \\
	P(\vb{b})                & = \prod_{l=1}^L \frac{\prod_r n_r^l!}{K^{l-1}!} \binom{ K^{l-1} - 1}{K^l - 1}^{-1} \frac{1}{K^{l-1}}
\end{align}
where $N$ is the total number of nodes, $M$ the total number of edges, $K$ the number of blocks, $\left( \binom{n}{k} \right) = \binom{n + k - 1}{k}$, and $(2n)!! = 2^n n!$.
See equations (23-25), (41-42) of ~\cite{peixoto17-01}, or (13-15), (19-20) of ~\cite{funke19-04}.

In graph-tool, this corresponds to the parameters
\begin{verbatim}
    NestedBlockState(g, b, deg_corr=False).entropy()
\end{verbatim}

\subsection{Microcanonical Degree Corrected (HDCPU)}

Let $A$ be the adjacency matrix, $\vb{e}$ the edge matrix between blocks, $\vb{k}$ the degree vector (per node), and $\vb{b}$ the block assignments (per node).
We optimize for minimum description length $\Sigma_{hdpcu} = -\log_2P(A, \vb{k}, \vb{e}, \vb{b}) = -\log_2P(A \mid \vb{k}, \vb{e}, \vb{b}) - \log_2 P(\vb{k} \mid \vb{e}, \vb{b}) - \log_2 P(\vb{e} \mid \vb{b}) - \log_2 P(\vb{b}) $, where
\begin{align}
	P(A \mid \vb{k}, \vb{e}, \vb{b}) & = \frac{ \prod_i k_i! \prod_{r < s} e_{rs}! \prod_{r} e_{rr}!!} {\prod_{i < j} A_{ij}! \prod_i A_{ii}!! \prod_r e_r!}                                      \\
	P(\vb{k} \mid \vb{e}, \vb{b})    & = \prod_r \left( \binom{n_r}{e_r} \right)^{-1}                                                                                                             \\
	P(\vb{e} \mid \vb{b})            & = \prod_{l=1}^{L} \prod_{r < s} \left( \binom{n_r^l n_s^l}{e_{rs}^{l+1}} \right)^{-1} \prod_r \left( \binom{ n_r^l (n_r^l + 1) / 2 }{e_{rr}^{l+1} }\right) \\
	P(\vb{b})                        & = \prod_{l=1}^L \frac{\prod_r n_r^l!}{K^{l-1}!} \binom{ K^{l-1} - 1}{K^l - 1}^{-1} \frac{1}{K^{l-1}}
\end{align}
where $N$ is the total number of nodes, $M$ the total number of edges, $K$ the number of blocks, $\left( \binom{n}{k} \right) = \binom{n + k - 1}{k}$, and $(2n)!! = 2^n n!$.
See equations (1-3), (26), (41-42) in ~\cite{peixoto17-01}, or (16-18) of ~\cite{funke19-04}.

In graph-tool, this corresponds to the parameters
\begin{verbatim}
    NestedBlockState(g, b, deg_corr=False, entropy_args={ degree_dl_kind = "uniform" }).entropy()
\end{verbatim}

\subsection{Microcanonical Degree Corrected (HDCPUH)}

Let $A$ be the adjacency matrix, $\vb{e}$ the edge matrix between blocks, $\vb{k}$ the degree vector (per node), and $\vb{b}$ the block assignments (per node).
We optimize for minimum description length $\Sigma_{hdpcuh} = -\log_2P(A, \vb{k}, \vb{e}, \vb{b}) = -\log_2P(A \mid \vb{k}, \vb{e}, \vb{b}) - \log_2 P(\vb{k} \mid \vb{e}, \vb{b}) - \log_2 P(\vb{e} \mid \vb{b}) - \log_2 P(\vb{b}) $, where
\begin{align}
	P(A \mid \vb{k}, \vb{e}, \vb{b}) & = \frac{ \prod_i k_i! \prod_{r < s} e_{rs}! \prod_{r} e_{rr}!!} {\prod_{i < j} A_{ij}! \prod_i A_{ii}!! \prod_r e_r!}                                      \\
	P(\vb{k} \mid \vb{e}, \vb{b})    & = \prod_r \frac{\prod_k N_k^r!}{n_r!} q(e_r, n_r)^{-1}                                                                                                     \\
	P(\vb{e} \mid \vb{b})            & = \prod_{l=1}^{L} \prod_{r < s} \left( \binom{n_r^l n_s^l}{e_{rs}^{l+1}} \right)^{-1} \prod_r \left( \binom{ n_r^l (n_r^l + 1) / 2 }{e_{rr}^{l+1} }\right) \\
	P(\vb{b})                        & = \prod_{l=1}^L \frac{\prod_r n_r^l!}{K^{l-1}!} \binom{ K^{l-1} - 1}{K^l - 1}^{-1} \frac{1}{K^{l-1}}
\end{align}
where $N_k^r$ denotes the number of nodes with degree $k$ in group $r$ and $q(m, n)$ the number of partitions of $m$ into $n$ parts.
That is,
\begin{equation}
	q(m, n) =
	\begin{cases}
		1                         & m = n = 0        \\
		0                         & n = 0, m > 0     \\
		q(m, m)                   & n > m            \\
		q(m, n - 1) + q(m - n, n) & \text{otherwise}
	\end{cases}
\end{equation}
See equations (1-3), (27-30), (42) in ~\cite{peixoto17-01}, or (16-18), (19-20) of ~\cite{funke19-04}.

In graph-tool, this corresponds to the parameters
\begin{verbatim}
    NestedBlockState(g, b, deg_corr=False, entropy_args={degree_dl_kind="distributed"}).entropy()
\end{verbatim}

\clearpage
\bibliography{refs}
\bibliographystyle{ACM-Reference-Format}

\end{document}
